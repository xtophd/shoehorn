## Authors: 
##   Christoph Doerbeck
##
## Summary:
##


- delegate_to: localhost
  block:

      ##
      ##    NOTE: All tasks here are blocked and delegated to excute
      ##          on the deployhost/localhost 
      ##


      ##
      ##    Determine state of the vm
      ##
    
    
    
      - name: "libvirt-deploy : determine state of vm"
        shell:
          cmd: "virsh domstate '{{ xtoph_deploy.libvirt.vm.name }}'"
        register: vminfo_result
        ignore_errors: true

      - name: "libvirt-deploy : DEBUG vm status/info results"
        debug: var=vminfo_result
        when: xtoph_deploy.debug == true
    
    
    
      ##
      ##    If the node is absent OR node NOT up, 
      ##    then set_fact to deploy the node
      ##
    
    
    
      - set_fact:
          deploy_node: false
      
      - set_fact:
          deploy_node: true
        when: vminfo_result.rc != 0
    
      - set_fact:
          deploy_node: true
        when: ( vminfo_result.rc == 0 and vminfo_result.stdout == "shut off" )

      - name:  "libvirt-deploy : DEBUG vm status result"
        debug:
          var: vminfo_result
        when: xtoph_deploy.debug == true

      - name:  "libvirt-deploy : DEBUG vm status/info results"
        debug:
          msg: "deploy_node = {{ deploy_node }}"
        when: xtoph_deploy.debug == true



##
##
##



- delegate_to: localhost
  block:



      ##
      ##    NOTE: All tasks here are blocked and delegated to excute
      ##          on the deployhost/localhost when deploy_node = true
      ##



      ##
      ##    Cleanup and create a fresh temp workspace
      ##
  
  
  
      - name: "libvirt-deploy : DEBUG temp workspace details"
        debug: 
          msg:
          - "temp workspace = {{ xtoph_deploy.deployhost.tmp_dir }}"
        when: xtoph_deploy.debug == true
  
      - name: "libvirt-deploy : delete old temp workspace"
        shell:
          cmd: |
            if [[ -d "{{ xtoph_deploy.deployhost.tmp_dir }}" ]] ; then rm -rf {{ xtoph_deploy.deployhost.tmp_dir }} ; fi
  
      - name: "libvirt-deploy : create new temp workspace"
        file:
          path="{{ xtoph_deploy.deployhost.tmp_dir }}/{{ item }}"
          mode="0755"
          state=directory
        loop:
          - iso
          - artifacts 
          - virt-install-fragments



      - block:
  

 
          ## 
          ##    All tasks in this block are related to 
          ##    kickstart repo and ks.cfg setup
          ##
          ##    * NOTE * to avoid concurrency problems when
          ##             when detecting and mounting the iso 
          ##             mount, we set 'throttle = 1'
          ##



          - name: "libvirt-deploy : DEBUG repo loopback details"
            debug: 
              msg:
               - "repo mnt = {{ xtoph_deploy.deployhost.repos.dir }}/{{ xtoph_deploy.kickstart_profile.mnt }}"
               - "repo src = {{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.repo_iso }}"
            when: xtoph_deploy.debug == true
  
          - name: "libvirt-deploy : stat repo mount point"
            throttle: 1
            stat: path="{{ xtoph_deploy.deployhost.repos.dir }}/{{ xtoph_deploy.kickstart_profile.mnt }}"
            register: test_repo_mount
            when: xtoph_deploy.kickstart_profile is defined and xtoph_deploy.kickstart_profile.repo_iso != ""
  
          - name: "libvirt-deploy : create repo mount point"
            throttle: 1
            file: 
              path:  "{{ xtoph_deploy.deployhost.repos.dir }}/{{ xtoph_deploy.kickstart_profile.mnt }}" 
              mode:  "0755"
              state: directory
            when: test_repo_mount.stat.exists == false
  
          - name: "libvirt-deploy : mount iso"
            throttle: 1
            vars:
            mount:
              path:   "{{ xtoph_deploy.deployhost.repos.dir }}/{{ xtoph_deploy.kickstart_profile.mnt }}"
              src:    "{{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.repo_iso }}"
              opts:   "loop,ro"
              state:  "mounted"
              fstype: "iso9660"
            when: xtoph_deploy.kickstart_profile.repo_iso != ""



          ##
          ##    Create the custom ISO for the OS installation
          ##
          ##    * NOTE * to avoid concurrency problems with
          ##             disk space and i/o bandwidth, we
          ##             use serial at the import_task level
          ##             DO NOT use "throttle" here as a stop gap
          ##


    
          - name: "libvirt-deploy : test iso availability"
            stat: 
              path: "{{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.boot_iso }}"
            register: iso_result
    
          - fail: 
              msg: "Specified ISO does not exist: {{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.boot_iso }}"
            when: not iso_result.stat.exists
    
          - name: "libvirt-deploy : clone ISO source to temp workspace"
            vars:
              t_isofile: "{{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.boot_iso }}"
              t_destdir: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso"
            shell:
              cmd: |
                xorriso -osirrox on -indev "{{ t_isofile }}" -extract / "{{ t_destdir }}"
    
    
    
          ##
          ##    Create the kickstart config
          ##
          ##    NOTE: put 1 copy in http directory
          ##          and 1 on the ISO
          ##
      
      
      
          - name: "libvirt-deploy : create kickstart http dir '{{ xtoph_deploy.deployhost.kickstart.dir }}'"
            file: 
              path: "{{ xtoph_deploy.deployhost.kickstart.dir }}" 
              mode: "0755" 
              state: directory
      
          - name: "libvirt-deploy : deploy kickstart config iso temp workspace and http dir"
            vars:
              p_ssh_key:    "{{ lookup('file','/root/.ssh/id_rsa.pub') }}"
              p_diskDevice: "{{ xtoph_deploy.hardware_profile.disk.dev }}"
            template:
              src:   "{{ xtoph_deploy.kickstart_profile.template }}"
              dest:  "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/ks.cfg"
              owner: root
              group: root
              mode: 0444
            loop:
              - "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/ks.cfg"
              - "{{ xtoph_deploy.deployhost.kickstart.dir }}/{{ g_clusterName }}-{{ inventory_hostname }}.cfg"
    
    
    
          ##
          ##    Adjust the isolinux.cfg
          ##
       
      
      
          - name: "libvirt-deploy : isolinux.cfg: change default timeout to 3 seconds"
            replace:
              path: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
              regexp: '^timeout(\s+.*)?$'
              replace: 'timeout 30'
      
          - name: "libvirt-deploy : remove default menu option from isolinux.cfg"
            lineinfile:
              dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
              regexp: "^.*menu default.*$"
              state:  absent
      
          - name: "libvirt-deploy : set new default menu option in isolinux.cfg"
            lineinfile:
              dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
              insertafter: "^.*label linux.*$"
              line: "  menu default"
              state: present
    
    
    
          ##
          ##    Kernel arguments for method = cdrom
          ##
    
    
    
          - name: "libvirt-deploy : isolinux.cfg: add kickstart and network info"
            replace:
              path: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
              regexp: '(\s+)append(\s+.*)?$'
              replace: '\1append\2 inst.ks=cdrom:/ks.cfg ip={{ xtoph_deploy.libvirt.network.net0.ip }} netmask={{ xtoph_deploy.libvirt.network.net0.netmask }} nameserver={{ xtoph_deploy.libvirt.network.net0.nameserver }} gateway={{ xtoph_deploy.libvirt.network.net0.gateway }}'
            when: xtoph_deploy.kickstart_profile.method == "cdrom"
    
    
    
          ##
          ##    Kernel arguments for method = network
          ##
      
      
  
          - name: "libvirt-deploy : isolinux.cfg: add kickstart and network info"
            replace:
              path: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
              regexp: '(\s+)append(\s+.*)?$'
              replace: '\1append\2 inst.ks=cdrom:/ks.cfg ip={{ xtoph_deploy.libvirt.network.net0.ip }} netmask={{ xtoph_deploy.libvirt.network.net0.netmask }} nameserver={{ xtoph_deploy.libvirt.network.net0.nameserver }} gateway={{ xtoph_deploy.libvirt.network.net0.gateway }} inst.repo={{ xtoph_deploy.deployhost.repos.url }}/{{ xtoph_deploy.kickstart_profile.mnt }}'
            when: xtoph_deploy.kickstart_profile.method == "network"
    
    
    
          ##
          ##    Preserve some files in the temp workspace
          ##    artifacts directory for easy debugging
          ##
      
      
          
          - name: "libvirt-deploy : copy files to temp workspace artifacts directory for preservation"
            copy:
              dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts"
              src:  "{{ item }}"
            loop:
              - "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg" 
              - "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/ks.cfg" 
           
          - name: "libvirt-deploy : save state of xtoph_deploy dictionary to artifacts directory"
            copy:
              dest:     "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/xtoph_deploy.yml"
              content:  "{{ xtoph_deploy | to_nice_yaml }}"
       
    
    
          ##
          ##    Generate the custom ISO image
          ##
   
 
    
          - name: "libvirt-deploy : create script to generate iso"
            template:
              src: "mkiso.j2"
              dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mkiso.sh"
              owner: root
              group: root
              mode: 0755
     
          - name: "libvirt-deploy : execute mkiso.sh script"
            shell: 
              cmd: |
                {{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mkiso.sh
    
    
    
          ##
          ##    Transfer the ISO image to libvirt storage domain
          ##
          ##    NOTE: to avoid concurrency problems with
          ##          disk space and i/o bandwidth, we
          ##          set 'throttle = 1'
          ##

          ##
          ##    NOTE: YAML Spec 1.2 http://www.yaml.org/spec/1.2/spec.html#id2760844
          ##          Scalar content can be written in block notation, 
          ##          using a literal style (indicated by “|”) where 
          ##          all line breaks are significant. Alternatively, 
          ##          they can be written with the folded style 
          ##          (denoted by “>”) where each line break is folded
          ##          to a space unless it ends an empty or a more-indented line.
          ## 

    
    
          - name: "libvirt-deploy : upload iso image"
            throttle: 1
            copy:
              src: "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.libvirt.vm.name }}.iso"
              dest: "{{ xtoph_deploy.libvirt.storage.qcow_dir }}"
      
          - name: "libvirt-deploy : clean-up temporary work-space"
            shell:
              cmd: |
                if [[ -d "{{ xtoph_deploy.deployhost.tmp_dir }}/iso" ]] ; then 
                  rm -rf "{{ xtoph_deploy.deployhost.tmp_dir }}/iso"
                fi
    
                if [[ -e "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.libvirt.vm.name }}.iso" ]] ; then 
                  rm -f "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.libvirt.vm.name }}.iso"
                fi
            when: xtoph_deploy.cleanup == true

        when: xtoph_deploy.kickstart_profile is defined and
              xtoph_deploy.kickstart_profile.method != "pxe"



      ##
      ##    Network Config Checks
      ##



      - name: "libvirt-deploy : network validate fwd_type"
        fail: msg="currently only support network.fwd_type = [ nat | bridge ]"
        when:
          - xtoph_deploy.libvirt.network.net0.fwd_type != "nat"
          - xtoph_deploy.libvirt.network.net0.fwd_type != "bridge"

      - name: "libvirt-deploy : network status check"
        script: |
            testnet-libvirt.sh "{{ xtoph_deploy.libvirt.network.net0.network_name }}"
        register: testnet_libvirt



      ##
      ##    Network XML Templates
      ##



      - name: "libvirt-deploy : xml to define libvirt-network as 'nat'"
        template:
          src: "libvirt-nat-xml.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/libvirt-network.xml"
          owner: root
          group: root
          mode: 0644
        when: xtoph_deploy.libvirt.network.net0.fwd_type == "nat"

      - name: "libvirt-deploy : xml to define libvirt-network as 'bridge'"
        template:
          src: "libvirt-bridge-xml.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/libvirt-network.xml"
          owner: root
          group: root
          mode: 0644
        when: xtoph_deploy.libvirt.network.net0.fwd_type == "bridge"


     
      ##
      ##    Scripts to create and remove network
      ##
      ##    NOTE: the creation of the REMOVE script rmnet-libvirt.sh
      ##          is only meant to serve for trouble shooting.  It is
      ##          NOT used at any point during a "deploy".  During
      ##          "undeploy", the script will be templated again.
      ##



      - name: "libvirt-deploy : script to create bridge network"
        vars:
        template:
          src: "mknet-bridge.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mknet-bridge.sh"
          owner: root
          group: root
          mode: 0755
        when: xtoph_deploy.libvirt.network.net0.fwd_type == "bridge"

      - name: "libvirt-deploy : script to create libvirt network"
        vars:
        template:
          src: "mknet-libvirt.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mknet-libvirt.sh"
          owner: root
          group: root
          mode: 0755
   
      - name: "libvirt-deploy : script to remove libvirt network"
        vars:
        template:
          src: "rmnet-libvirt.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/rmnet-libvirt.sh"
          owner: root
          group: root
          mode: 0755
   

 
      ##
      ##    Create virt-install script
      ##



      - name: "libvirt-deploy : virt-install base fragment"
        template:
          src:  virt-install-base.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/10-base"

      - name: "libvirt-deploy : virt-install extra disk fragments"
        vars:
          p_disksize: "{{ xtoph_deploy['resource_profile']['storage']['extra'][t_diskname].size }}"
        template:
          src: virt-install-disk.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/30-extra-{{ t_diskname }}"
        loop: "{{ xtoph_deploy.resource_profile.storage.extra | flatten(1) }}"
        loop_control:
          loop_var: t_diskname
        when: xtoph_deploy.resource_profile.storage.extra is defined

      - name: "libvirt-deploy: virt-install pxe fragment"
        template:
          src:  virt-install-pxe.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/35-pxe"
        when: xtoph_deploy.kickstart_profile.method == "pxe"

      - name: "libvirt-deploy: virt-install cdrom fragment"
        template:
          src:  virt-install-cdrom.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/40-cdrom"
        when: xtoph_deploy.kickstart_profile.method == "cdrom" or
              xtoph_deploy.kickstart_profile.method == "network"

      - name: "libvirt-deploy: virt-install finish fragment"
        template:
          src:  virt-install-finish.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/50-finish"

      - name: "libvirt-deploy : virt-install assemble fragments"
        assemble:
          src: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/virtinstall.sh"
          owner: root
          group: root
          mode: 0744
   

 
      ##
      ##    Execute virtual machine installation scripts
      ##



      - name: "libvirt-deploy : create bridge network"
        throttle: 1
        shell:
          cmd: |
            bash "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mknet-bridge.sh"
        when: xtoph_deploy.libvirt.network.net0.fwd_type == "bridge"

      - name: "libvirt-deploy : create libvirt network"
        throttle: 1
        shell:
          cmd: |
            bash "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mknet-libvirt.sh"

      - name: "libvirt-deploy : create virtual machine"
        throttle: 1
        shell:
          cmd: |
            bash "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/virtinstall.sh"

      - name: "libvirt-deploy-monitor : wait for virtual machine running"
        shell:
          cmd: |
            virsh domstate "{{ xtoph_deploy.libvirt.vm.name }}" | grep -q "running"
        register: result
        until:  result.rc == 0
        retries: 600
        delay: 5


  ##
  ## End - block:
  ##

  when: deploy_node == true


