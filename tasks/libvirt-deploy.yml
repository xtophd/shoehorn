## Authors: 
##   Christoph Doerbeck
##
## Summary:
##


- delegate_to: localhost
  block:

      ##
      ##    NOTE: All tasks here are blocked and delegated to excute
      ##          on the deployhost/localhost 
      ##


      ##
      ##    Determine state of the vm
      ##
    
    
    
      - name: "libvirt-deploy : determine state of vm"
        shell:
          cmd: "virsh domstate '{{ xtoph_deploy.libvirt.vm.name }}' | head -n 1"
        register: vminfo_result
        ignore_errors: true

      - name: "libvirt-deploy : DEBUG vm status/info results"
        debug: var=vminfo_result
        when: xtoph_deploy.debug == true
    
    
    
      ##
      ##    If the node is absent OR node NOT up, 
      ##    then set_fact to deploy the node
      ##
    
    
    
      - set_fact:
          deploy_node: false
      
      - set_fact:
          deploy_node: true
        when: vminfo_result.rc != 0
    
      - set_fact:
          deploy_node: true
        when: ( vminfo_result.rc == 0 and vminfo_result.stdout == "shut off" )
    
      - name:  "libvirt-deploy : DEBUG vm status/info results"
        debug:
          msg: "deploy_node = {{ deploy_node }}"
        when: xtoph_deploy.debug == true



##
##
##



- delegate_to: localhost
  block:

      ##
      ##    NOTE: All tasks here are blocked and delegated to excute
      ##          on the deployhost/localhost when deploy_node = true
      ##

    
      ##
      ##    Update /etc/hosts with hostnames and IPs of new cluster
      ##
      ##    NOTE: to avoid cuncurrency problems editing a single
      ##          file, we set 'throttle = 1'
      ##



      - name: "libvirt-deploy : cleanup conflicting IP in /etc/hosts"
        throttle: 1
        lineinfile:
          dest:   "/etc/hosts"
          regexp: "{{ h_pubIP }} .*$"
          state:  absent
      
      - name: "libvirt-deploy : cleanup conflicting name in /etc/hosts"
        throttle: 1
        lineinfile:
          dest:   "/etc/hosts"
          regexp: ".*{{ inventory_hostname }}.{{ g_network0_fqdn }}.*"
          state:  absent
      
      - name: "libvirt-deploy : add entry in /etc/hosts"
        throttle: 1
        lineinfile:
          dest:   "/etc/hosts"
          line:   "{{ h_pubIP }} {{ inventory_hostname }}.{{ g_network0_fqdn }}"
          state:  present



      ##
      ##    Create MAC addresses if not already configured
      ##    and store it in the ../config/host_vars directory
      ##
 


      - name: "libvirt-deploy : create host_vars dir"
        file: path="{{ lookup('env','PWD') }}/config/host_vars" state=directory
   
      - name: "libvirt-deploy : create host_vars file"
        file: path="{{ lookup('env','PWD') }}/config/host_vars/{{ inventory_hostname }}" state=touch
    
      - name: "libvirt-deploy : generate random MAC "
        script: macgen.py
        register: macgen_output
        when: h_pubMAC is undefined
    
      - name: "libvirt-deploy : store random MAC in host_vars"
        lineinfile:
          dest:   "{{ lookup('env', 'PWD') }}/config/host_vars/{{ inventory_hostname }}"
          line:   "h_pubMAC: '{{ macgen_output.stdout }}'"
          state:  present
        when: h_pubMAC is undefined
        register: makeupmac
    
      - name: "libvirt-deploy : set fact to preserve MAC for this run"
        set_fact:
          h_pubMAC: '{{ macgen_output.stdout }}'
        when: makeupmac.changed



      ##
      ##    Set up a repo using a loopback mounted ISO
      ##
      ##    * NOTE * to avoid concurrency problems when
      ##             when detecting and mounting the iso 
      ##             mount, we set 'throttle = 1'
      ##
  
  
  
      - name: "libvirt-deploy : DEBUG repo loopback details"
        debug: 
          msg:
           - "repo mnt = {{ xtoph_deploy.deployhost.repos.dir }}/{{ xtoph_deploy.kickstart_profile.mnt }}"
           - "repo src = {{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.repo_iso }}"
        when: xtoph_deploy.debug == true
  
      - name: "libvirt-deploy : stat repo mount point"
        throttle: 1
        stat: path="{{ xtoph_deploy.deployhost.repos.dir }}/{{ xtoph_deploy.kickstart_profile.mnt }}"
        register: test_repo_mount
        when: xtoph_deploy.kickstart_profile.repo_iso != ""
  
      - name: "libvirt-deploy : create repo mount point"
        throttle: 1
        file: 
          path:  "{{ xtoph_deploy.deployhost.repos.dir }}/{{ xtoph_deploy.kickstart_profile.mnt }}" 
          mode:  "0755"
          state: directory
        when: test_repo_mount.stat.exists == false
  
      - name: "libvirt-deploy : mount iso"
        throttle: 1
        vars:
        mount:
          path:   "{{ xtoph_deploy.deployhost.repos.dir }}/{{ xtoph_deploy.kickstart_profile.mnt }}"
          src:    "{{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.repo_iso }}"
          opts:   "loop,ro"
          state:  "mounted"
          fstype: "iso9660"
        when: xtoph_deploy.kickstart_profile.repo_iso != ""



      ##
      ##    Cleanup and create a fresh temp workspace
      ##
  
  
  
      - name: "libvirt-deploy : DEBUG temp workspace details"
        debug: 
          msg:
          - "temp workspace = {{ xtoph_deploy.deployhost.tmp_dir }}"
        when: xtoph_deploy.debug == true
  
      - name: "libvirt-deploy : delete old temp workspace"
        shell:
          cmd: |
            if [[ -d "{{ xtoph_deploy.deployhost.tmp_dir }}" ]] ; then rm -rf {{ xtoph_deploy.deployhost.tmp_dir }} ; fi
  
      - name: "libvirt-deploy : create new temp workspace"
        file:
          path="{{ xtoph_deploy.deployhost.tmp_dir }}/{{ item }}"
          mode="0755"
          state=directory
        loop:
          - iso
          - artifacts 
          - network



      ##
      ##    Create the custom ISO for the OS installation
      ##
      ##    * NOTE * to avoid concurrency problems with
      ##             disk space and i/o bandwidth, we
      ##             use serial at the import_task level
      ##             DO NOT use "throttle" here as a stop gap
      ##



      - name: "libvirt-deploy : test iso availability"
        stat: 
          path: "{{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.boot_iso }}"
        register: iso_result

      - fail: 
          msg: "Specified ISO does not exist: {{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.boot_iso }}"
        when: not iso_result.stat.exists

      - name: "libvirt-deploy : clone ISO source to temp workspace"
        vars:
          t_isofile: "{{ xtoph_deploy.deployhost.iso.dir }}/{{ xtoph_deploy.kickstart_profile.boot_iso }}"
          t_destdir: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso"
        shell:
          cmd: |
            xorriso -osirrox on -indev "{{ t_isofile }}" -extract / "{{ t_destdir }}"



      ##
      ##    Create the kickstart config
      ##
      ##    NOTE: put 1 copy in http directory
      ##          and 1 on the ISO
      ##
  
  
  
      - name: "libvirt-deploy : create kickstart http dir '{{ xtoph_deploy.deployhost.kickstart.dir }}'"
        file: 
          path: "{{ xtoph_deploy.deployhost.kickstart.dir }}" 
          mode: "0755" 
          state: directory
  
      - name: "libvirt-deploy : deploy kickstart config iso temp workspace and http dir"
        vars:
          p_ssh_key:    "{{ lookup('file','/root/.ssh/id_rsa.pub') }}"
          p_diskDevice: "{{ xtoph_deploy.hardware_profile.disk.dev }}"
        template:
          src:   "{{ xtoph_deploy.kickstart_profile.template }}"
          dest:  "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/ks.cfg"
          owner: root
          group: root
          mode: 0444
        loop:
          - "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/ks.cfg"
          - "{{ xtoph_deploy.deployhost.kickstart.dir }}/{{ g_clusterName }}-{{ inventory_hostname }}.cfg"



      ##
      ##    Adjust the isolinux.cfg
      ##
   
  
  
      - name: "libvirt-deploy : isolinux.cfg: change default timeout to 3 seconds"
        replace:
          path: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
          regexp: '^timeout(\s+.*)?$'
          replace: 'timeout 30'
  
      - name: "libvirt-deploy : remove default menu option from isolinux.cfg"
        lineinfile:
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
          regexp: "^.*menu default.*$"
          state:  absent
  
      - name: "libvirt-deploy : set new default menu option in isolinux.cfg"
        lineinfile:
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
          insertafter: "^.*label linux.*$"
          line: "  menu default"
          state: present



      ##
      ##    Kernel arguments for method = cdrom
      ##



      - name: "libvirt-deploy : isolinux.cfg: add kickstart and network info"
        replace:
          path: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
          regexp: '(\s+)append(\s+.*)?$'
          replace: '\1append\2 inst.ks=cdrom:/ks.cfg ip={{ xtoph_deploy.libvirt.network.net0.ip }} netmask={{ xtoph_deploy.libvirt.network.net0.netmask }} nameserver={{ xtoph_deploy.libvirt.network.net0.nameserver }} gateway={{ xtoph_deploy.libvirt.network.net0.gateway }}'
        when: xtoph_deploy.kickstart_profile.method == "cdrom"



      ##
      ##    Kernel arguments for method = network
      ##
  
  
  
      - name: "libvirt-deploy : isolinux.cfg: add kickstart and network info"
        replace:
          path: "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg"
          regexp: '(\s+)append(\s+.*)?$'
          replace: '\1append\2 inst.ks=cdrom:/ks.cfg ip={{ xtoph_deploy.libvirt.network.net0.ip }} netmask={{ xtoph_deploy.libvirt.network.net0.netmask }} nameserver={{ xtoph_deploy.libvirt.network.net0.nameserver }} gateway={{ xtoph_deploy.libvirt.network.net0.gateway }} inst.repo={{ xtoph_deploy.deployhost.repos.url }}/{{ xtoph_deploy.kickstart_profile.mnt }}'
        when: xtoph_deploy.kickstart_profile.method == "network"



      ##
      ##    Preserve some files in the temp workspace
      ##    artifacts directory for easy debugging
      ##
  
  
      
      - name: "libvirt-deploy : copy files to temp workspace artifacts directory for preservation"
        copy:
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts"
          src:  "{{ item }}"
        loop:
          - "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/isolinux/isolinux.cfg" 
          - "{{ xtoph_deploy.deployhost.tmp_dir }}/iso/ks.cfg" 
       
      - name: "libvirt-deploy : save state of xtoph_deploy dictionary to artifacts directory"
        copy:
          dest:     "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/xtoph_deploy.yml"
          content:  "{{ xtoph_deploy | to_nice_yaml }}"
   


      ##
      ##    Generate the custom ISO image
      ##


  
      - name: "libvirt-deploy : generate custom iso image"
        shell:
          cmd: |
            cd {{ xtoph_deploy.deployhost.tmp_dir }}/iso
            genisoimage -U -r -v -T -J -joliet-long        \
                        -V      "{{ xtoph_deploy.kickstart_profile.boot_iso_label }}"        \
                        -volset "{{ xtoph_deploy.kickstart_profile.boot_iso_label }}"        \
                        -A      "{{ xtoph_deploy.kickstart_profile.boot_iso_label }}"        \
                        -b      isolinux/isolinux.bin      \
                        -c      isolinux/boot.cat          \
                        -no-emul-boot                      \
                        -boot-load-size 4                  \
                        -boot-info-table                   \
                        -eltorito-alt-boot                 \
                        -e images/efiboot.img              \
                        -no-emul-boot                      \
                        -o      "../{{ xtoph_deploy.libvirt.vm.name }}.iso" .


      ##  
      ##    NOTE: I have not made use of a hybrid
      ##          ISO for xtoph_deploy, but why not
      ##          allow hybrid booting as a CD-ROM or as a
      ##          hard disk with our generated image



      - name: "libvirt-deploy : enable hybrid uefi iso"
        shell:
          cmd: |
            isohybrid --uefi "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.libvirt.vm.name }}.iso"
  
  
  
      ##
      ##    Transfer the ISO image to libvirt storage domain
      ##
      ##    NOTE: to avoid concurrency problems with
      ##          disk space and i/o bandwidth, we
      ##          set 'throttle = 1'
      ##


  
      - name: "libvirt-deploy : upload iso image"
        copy:
          src: "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.libvirt.vm.name }}.iso"
          dest: "{{ xtoph_deploy.libvirt.storage.qcow_dir }}"
  
      - name: "libvirt-deploy : clean-up temporary work-space"
        shell:
          cmd: |
            if [[ -d "{{ xtoph_deploy.deployhost.tmp_dir }}/iso" ]] ; then rm -rf "{{ xtoph_deploy.deployhost.tmp_dir }}/iso" ; fi
            if [[ -e "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.libvirt.vm.name }}.iso" ]] ; then rm -f "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.libvirt.vm.name }}.iso" ; fi
        when: xtoph_deploy.cleanup == true



      ##
      ##    Network Configuration
      ##



      - name: "libvirt-deploy : network test existence"
        shell:
          cmd: |
            virsh net-info "{{ xtoph_deploy.libvirt.network.net0.network_name }}"
        ignore_errors: yes
        register: test_libvirt_network

      - name: "libvirt-deploy : network test fwd_type"
        fail: msg="currently only support network.fwd_type = [ nat | bridge ]"
        when:
          - xtoph_deploy.libvirt.network.net0.fwd_type != "nat"
          - xtoph_deploy.libvirt.network.net0.fwd_type != "bridge"



      ##
      ##    Network Templates
      ##



      - name: "libvirt-deploy : network xml template for 'nat'"
        vars:
          - p_name:    "{{ xtoph_deploy.libvirt.network.net0.network_name }}"
          - p_type:    "{{ xtoph_deploy.libvirt.network.net0.fwd_type }}"
          - p_address: "{{ xtoph_deploy.libvirt.network.net0.gateway }}"
          - p_netmask: "{{ xtoph_deploy.libvirt.network.net0.netmask }}"
        template:
          src: "libvirt-net-nat.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/network/libvirt-network.xml"
          owner: root
          group: root
          mode: 0644
        when: xtoph_deploy.libvirt.network.net0.fwd_type == "nat"

      - name: "libvirt-deploy : network xml template for 'bridge'"
        vars:
          - p_name:    "{{ xtoph_deploy.libvirt.network.net0.network_name }}"
          - p_type:    "{{ xtoph_deploy.libvirt.network.net0.fwd_type }}"
          - p_address: "{{ xtoph_deploy.libvirt.network.net0.gateway }}"
          - p_netmask: "{{ xtoph_deploy.libvirt.network.net0.netmask }}"
        template:
          src: "libvirt-net-bridge.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/network/libvirt-network.xml"
          owner: root
          group: root
          mode: 0644
        when: xtoph_deploy.libvirt.network.net0.fwd_type == "bridge"








  
      ##
      ##    Create virtual machine
      ##
      ##    NOTE:  the kickstart_profile can override the
      ##           hardware_profile default os_type.  This
      ##           provides easy customization of kvm
      ##           optimizations per OS
      ##



#    - name: "libvirt-deploy : create vm with cpu, mem and nics"
#      ovirt_vm:
#        auth:      "{{ ovirt_auth }}"
#        name:      "{{ xtoph_deploy.libvirt.vm.name }}"
#        cluster:   "{{ xtoph_deploy.libvirt.cluster_name }}"
#        memory:    "{{ xtoph_deploy.resource_profile.memsize }} MiB"
#        cpu_cores: "{{ xtoph_deploy.resource_profile.vcpus }}"
#        nics:
#          - name: "nic1"
#            interface: "{{ xtoph_deploy.hardware_profile.network.model }}"
#            profile_name: "{{ xtoph_deploy.libvirt.network.net0.libvirt_network }}"
#            mac_address: "{{ h_pubMAC }}"
#        graphical_console:
#           protocol:
#             - spice
#        operating_system: "{{ xtoph_deploy.kickstart_profile.kvm_os_type | default(xtoph_deploy.hardware_profile.default_os_type) }}"
#        high_availability: false
#        type: server
#        state: stopped
#        wait: yes



    ##
    ##    Add boot disk to virtual machine
    ##



#    - name: "libvirt-deploy : modify vm with boot-disk"
#      libvirt:
#        auth:              "{{ ovirt_auth }}"
#        name:              "{{ xtoph_deploy.ovirt.vm.name }}_root"
#        vm_name:           "{{ xtoph_deploy.ovirt.vm.name }}"
#        storage_domain:    "{{ xtoph_deploy.ovirt.storage.domain }}"
#        interface:         "{{ xtoph_deploy.hardware_profile.disk.bus }}"
#        sparse:            "{{ xtoph_deploy.hardware_profile.disk.sparse }}"
#        size:              "{{ xtoph_deploy.resource_profile.storage.root.size }} GiB"
#        bootable:          yes
#        format:            cow
#        wipe_after_delete: yes
#        wait:              yes

#    - name: "libvirt-deploy : modify vm with extra disks"
#      ovirt_disk:
#        auth:              "{{ ovirt_auth }}"
#        name:              "{{ xtoph_deploy.ovirt.vm.name }}_{{ t_diskname }}"
#        vm_name:           "{{ xtoph_deploy.ovirt.vm.name }}"
#        storage_domain:    "{{ xtoph_deploy.ovirt.storage.domain }}"
#        interface:         "{{ xtoph_deploy.hardware_profile.disk.bus }}"
#        sparse:            "{{ xtoph_deploy.hardware_profile.disk.sparse }}"
#        size:              "{{ xtoph_deploy['resource_profile']['storage']['extra'][t_diskname].size }} GiB"
#        bootable:          no
#        format:            cow
#        wipe_after_delete: yes
#        wait:              yes
#      loop: "{{ xtoph_deploy.resource_profile.storage.extra | flatten(1) }}"
#      loop_control:
#        loop_var: t_diskname
#      when: xtoph_deploy.resource_profile.storage.extra is defined



    ##
    ##
    ##    Configure boot device based on ISO availability
    ##
    ##



#    - name: "libvirt-deploy : vm attach ISO and change boot device to cdrom"
#      ovirt_vm:
#        auth:         "{{ ovirt_auth }}"
#        name:         "{{ xtoph_deploy.ovirt.vm.name }}"
#        cluster:      "{{ xtoph_deploy.ovirt.cluster_name }}"
#        cd_iso:       "{{ xtoph_deploy.ovirt.vm.name }}.iso"
#        boot_devices:
#          - cdrom
#          - hd
#      when:
#        - xtoph_deploy.kickstart_profile is defined
#
#    - name: "libvirt-deploy : vm change boot device to network"
#      ovirt_vm:
#        auth:         "{{ ovirt_auth }}"
#        name:         "{{ xtoph_deploy.ovirt.vm.name }}"
#        cluster:      "{{ xtoph_deploy.ovirt.cluster_name }}"
#        boot_devices:
#          - hd
#          - network
#      when:
#        - xtoph_deploy.kickstart_profile is not defined

#    - name: "libvirt-deploy : vm change state to RUNNING (START INSTALL)"
#      ovirt_vm:
#        auth: "{{ ovirt_auth }}"
#        name: "{{ xtoph_deploy.ovirt.vm.name }}"
#        cluster: "{{ xtoph_deploy.ovirt.cluster_name }}"
#        state: running
#        wait: no
#
#  when: deploy_node == true

