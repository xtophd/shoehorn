## Authors: 
##   Christoph Doerbeck
##
## Summary:
##


- name: "libvirt-deploy : BEGIN"
  delegate_to: localhost
  block:

      ##
      ##    NOTE: All tasks here are blocked and delegated to execute
      ##          on the localhost (deployhost/controller).  Some tasks
      ##          will be explicitly delegated to the libvirt platform(s)
      ##

      ##
      ##    Clean up the temp workspace
      ##

      - name: "libvirt-deploy : delete prior temp workspaces"
        file:
          path="{{ xtoph_deploy.deployhost.tmp_dir }}/{{ item }}"
          state=absent
        loop:
          - virt-install-fragments

      - name: "libvirt-deploy : create fresh temp workspaces"
        file:
          path="{{ xtoph_deploy.deployhost.tmp_dir }}/{{ item }}"
          mode="0755"
          state=directory
        loop:
          - virt-install-fragments



      ##
      ##    Transfer the ISO image to libvirt storage domain
      ##
      ##    NOTE: to avoid concurrency problems with
      ##          disk space and i/o bandwidth, we
      ##          set 'throttle = 1'
      ##

      - block:

            - name: "ovirt-deploy : DEBUG info about image transfer"
              debug:
                msg:
                  - "## libVIRT IMAGE TRANSFER DETAILS"
                  - "hostname: {{ xtoph_deploy.platform_profile.vm.name }}"
                  - "source: {{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.platform_profile.vm.name }}.iso"
                  - "destination: {{ xtoph_deploy.platform_profile.host_fqdn }}:{{ xtoph_deploy.platform_profile.storage.default.qcow_dir }}/{{ xtoph_deploy.platform_profile.vm.name }}.iso"
              when: xtoph_deploy.debug == true

            - name: "libvirt-deploy : upload iso image"
              throttle: 1
              delegate_to: "{{ xtoph_deploy.platform_profile.host_fqdn }}"
              copy:
                src: "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.platform_profile.vm.name }}.iso"
                dest: "{{ xtoph_deploy.platform_profile.storage.default.qcow_dir }}"

            - name: "libvirt-deploy : clean-up temporary work-space"
              shell:
                cmd: |
                  if [[ -d "{{ xtoph_deploy.deployhost.tmp_dir }}/iso" ]] ; then
                    rm -rf "{{ xtoph_deploy.deployhost.tmp_dir }}/iso"
                  fi
  
                  if [[ -e "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.platform_profile.vm.name }}.iso" ]] ; then
                    rm -f "{{ xtoph_deploy.deployhost.tmp_dir }}/{{ xtoph_deploy.platform_profile.vm.name }}.iso"
                  fi
              when: xtoph_deploy.cleanup == true

        when: xtoph_deploy.kickstart_profile is defined and
              xtoph_deploy.kickstart_profile.method != "pxe"



      ##
      ##    Network Config Checks
      ##

      - name: "libvirt-deploy : network validate fwd_type"
        fail: msg="currently only support network.fwd_type = [ nat | bridge | macvtap ]"
        when:
          - xtoph_deploy.platform_profile.network.default.fwd_type != "nat"
          - xtoph_deploy.platform_profile.network.default.fwd_type != "bridge"
          - xtoph_deploy.platform_profile.network.default.fwd_type != "macvtap"



      ##
      ##    Network XML Templates
      ##

      - name: "libvirt-deploy : xml to define libvirt-network using 'nat'"
        template:
          src: "libvirt-nat-xml.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/{{ xtoph_deploy.machine_profile.network.default.network_name }}-network.xml"
          owner: root
          group: root
          mode: 0644
        when: xtoph_deploy.machine_profile.network.default.fwd_type == "nat"

      - name: "libvirt-deploy : xml to define libvirt-network using 'bridge'"
        template:
          src: "libvirt-bridge-xml.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/{{ xtoph_deploy.machine_profile.network.default.network_name }}-network.xml"
          owner: root
          group: root
          mode: 0644
        when: xtoph_deploy.machine_profile.network.default.fwd_type == "bridge"

      - name: "libvirt-deploy : xml to define libvirt-network using 'macvtap'"
        template:
          src: "libvirt-macvtap-xml.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/{{ xtoph_deploy.machine_profile.network.default.network_name }}-network.xml"
          owner: root
          group: root
          mode: 0644
        when: xtoph_deploy.machine_profile.network.default.fwd_type == "macvtap"


     
      ##
      ##    Scripts to create and remove network
      ##
      ##    NOTE: the creation of the REMOVE script rmnet-libvirt.sh
      ##          is only meant to serve for trouble shooting.  It is
      ##          NOT used at any point during a "deploy".  During
      ##          "undeploy", the script will be templated again.
      ##

      - name: "libvirt-deploy : script to create libvirt network"
        vars:
        template:
          src: "mknet-libvirt.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mknet-libvirt.sh"
          owner: root
          group: root
          mode: 0755

      - name: "libvirt-deploy : script to remove libvirt network"
        vars:
        template:
          src: "rmnet-libvirt.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/rmnet-libvirt.sh"
          owner: root
          group: root
          mode: 0755
   
      - name: "libvirt-deploy : script to create extra bridge network"
        vars:
        template:
          src: "mknet-bridge.j2"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mknet-bridge.sh"
          owner: root
          group: root
          mode: 0755
        when: xtoph_deploy.platform_profile.network.default.fwd_type == "bridge"


 
      ##
      ##    Create virt-install script
      ##

      - name: "libvirt-deploy : some facts about our platform host"
        debug:
          msg:
            - "{{ hostvars[xtoph_deploy.platform_profile.host_fqdn].ansible_distribution }}"
            - "{{ hostvars[xtoph_deploy.platform_profile.host_fqdn].ansible_distribution_major_version }}"
            - "{{ hostvars[xtoph_deploy.platform_profile.host_fqdn].ansible_distribution_version }}"

      - name: "libvirt-deploy : virt-install base fragment"
        template:
          src:  virt-install-base.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/10-base"
        when:
          - hostvars[xtoph_deploy.platform_profile.host_fqdn].ansible_distribution_major_version is version('9',operator='<')

      - name: "libvirt-deploy : virt-install base fragment"
        template:
          src:  virt-install-base-rhel9.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/10-base"
        when:
          - hostvars[xtoph_deploy.platform_profile.host_fqdn].ansible_distribution_major_version is version('9',operator='>=')

      - name: "libvirt-deploy : virt-install network fragments"
        vars:
          t_nic_profile: "{{ xtoph_deploy.resource_profile.network.extra[t_nicname].hw_network_profile | default('default') }}"
        template:
          src: virt-install-nic.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/20-extra-nic-{{ t_nicname }}"
        loop: "{{ xtoph_deploy.resource_profile.network.extra | flatten(1) }}"
        loop_control:
          loop_var: t_nicname
        when: xtoph_deploy.resource_profile.network.extra is defined

      - name: "libvirt-deploy : virt-install extra disk fragments"
        vars:
          t_disksize:     "{{ xtoph_deploy.resource_profile.storage.extra[t_diskname].size }}"
          t_disk_profile: "{{ xtoph_deploy.resource_profile.storage.extra[t_diskname].hw_storage_profile | default('default') }}"
        template:
          src: virt-install-disk.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/30-extra-disk-{{ t_diskname }}"
        loop: "{{ xtoph_deploy.resource_profile.storage.extra | flatten(1) }}"
        loop_control:
          loop_var: t_diskname
        when: xtoph_deploy.resource_profile.storage.extra is defined

      - name: "libvirt-deploy: virt-install pxe fragment"
        template:
          src:  virt-install-pxe.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/40-pxe"
        when: xtoph_deploy.kickstart_profile.method == "pxe"

      - name: "libvirt-deploy: virt-install cdrom fragment"
        template:
          src:  virt-install-cdrom.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/50-cdrom"
        when: xtoph_deploy.kickstart_profile.method == "cdrom" or
              xtoph_deploy.kickstart_profile.method == "simple_cdrom" or
              xtoph_deploy.kickstart_profile.method == "network"

      - name: "libvirt-deploy: virt-install finish fragment"
        template:
          src:  virt-install-finish.j2
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/60-finish"

      - name: "libvirt-deploy : virt-install assemble fragments"
        assemble:
          src: "{{ xtoph_deploy.deployhost.tmp_dir }}/virt-install-fragments/"
          dest: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/virtinstall.sh"
          owner: root
          group: root
          mode: 0744
   

 
      ##
      ##    Execute virtual machine installation scripts
      ##

      - name: "libvirt-deploy : copy xml network config to platform host"
        throttle: 1
        delegate_to: "{{ xtoph_deploy.platform_profile.host_fqdn }}"
        copy:
          dest: "/var/tmp/"
          src:  "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/{{ xtoph_deploy.machine_profile.network.default.network_name }}-network.xml"

      - name: "libvirt-deploy : create extra bridge network"
        throttle: 1
        delegate_to: "{{ xtoph_deploy.platform_profile.host_fqdn }}"
        script:
          cmd: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mknet-bridge.sh"
        register: mknet_results
        changed_when: "'CHANGED' in mknet_results.stdout"
        notify:
          - delegated_restart_dnsmasq
        when: xtoph_deploy.platform_profile.network.default.fwd_type == "bridge"

      - name: "libvirt-deploy : create libvirt network"
        throttle: 1
        delegate_to: "{{ xtoph_deploy.platform_profile.host_fqdn }}"
        script:
          cmd: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/mknet-libvirt.sh"
        register: mknet_results
        changed_when: "'CHANGED' in mknet_results.stdout"
        notify:
          - delegated_restart_dnsmasq

      - name: "libvirt-deploy-monitor : cleanup xml network config on platform host"
        delegate_to: "{{ xtoph_deploy.platform_profile.host_fqdn }}"
        file:
          path:  "/var/tmp/{{ xtoph_deploy.machine_profile.network.default.network_name }}-network.xml"
          state: absent
        when: xtoph_deploy.cleanup == true



  ##
  ##    flush_handlers does not support 'when', so we need
  ##    to exit the block
  ##

  ##
  ## End - block:
  ##

  when: deploy_node == true



##
##    Because we may have just created a new network bridge
##    we need flush looming handlers now
##



- name: "Flush handlers"
  delegate_to: localhost
  meta: flush_handlers



##
## Now resume block
##



- delegate_to: localhost
  block:

      ##
      ##    Create the virtual machine
      ##


      - name: "libvirt-deploy : create virtual machine"
        throttle: 1
        delegate_to: "{{ xtoph_deploy.platform_profile.host_fqdn }}"
        script:
          cmd: "{{ xtoph_deploy.deployhost.tmp_dir }}/artifacts/virtinstall.sh"

      - name: "libvirt-deploy-monitor : wait for virtual machine running (wait_for_powerup)"
        delegate_to: "{{ xtoph_deploy.platform_profile.host_fqdn }}"
        shell:
          cmd: |
            virsh domstate "{{ xtoph_deploy.platform_profile.vm.name }}" | grep -q "running"
        register: result
        until:  result.rc == 0
        retries: 600
        delay: 5
        when:
          - xtoph_deploy.kickstart_profile.wait_for_powerup is defined
          - xtoph_deploy.kickstart_profile.wait_for_powerup == true



  ##
  ## End - block:
  ##

  when: deploy_node == true



